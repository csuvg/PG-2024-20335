El objetivo central de este proyecto fue el desarrollo de una aplicación móvil que utilizó tecnología avanzada para la traducción en tiempo real de lengua de señas a texto y voz, facilitando así la comunicación para personas con discapacidad auditiva. Esta aplicación se valió de visión por computadora para detectar gestos y señas de manera efectiva y de tecnologías de inteligencia artificial para convertir estas señas a lenguaje natural, asegurando una interacción fluida y comprensible.

Adicionalmente, se diseñó una infraestructura de red dedicada que permitió no solo la comunicación efectiva entre la aplicación y el servidor central, sino también la gestión eficiente del flujo de datos. Esta infraestructura fue clave para recibir, procesar y responder a los videos enviados por los usuarios, implementando procesos que iban desde la interpretación de imagen hasta el procesamiento por modelos de inteligencia artificial, culminando con la entrega de la traducción al dispositivo del usuario.

Con el objetivo de hacer un proyecto accesible y que tuviera la oportunidad de crecer en el futuro, se buscó que la arquitectura de red y el servidor fueran seguros, eficientes y que tuvieran la capacidad de escalar. Esto se logró con el uso de APIs robustas y de máquinas virtuales que sirvieron para administrar el uso de recursos de la mejor manera posible.